# -*- coding: utf-8 -*-
"""STP598: Project Code Model 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uwH1NxgbP42ROYSrQo0wrbNIkp_R5Rol
"""

from google.colab import drive
drive.mount('/content/drive')

# Import necessary libraries

import matplotlib.pyplot as plt
from PIL import Image
import pandas as pd
import numpy as np
import os
import seaborn as sns
from random import shuffle
from tqdm import *
import tensorflow as tf
from sklearn.metrics import confusion_matrix
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Activation, Dense
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.optimizers import SGD

# Load image from file

image = Image.open("/content/drive/MyDrive/STP598 Project dataset/archive/amer_sign2.png")
image.show()

# Load image from file

image = Image.open("/content/drive/MyDrive/STP598 Project dataset/archive/american_sign_language.PNG")
image.show()

# Load training and test datasets from CSV files

Train_data = pd.read_csv('/content/drive/MyDrive/STP598 Project dataset/archive/sign_mnist_train.csv')
Test_data = pd.read_csv('/content/drive/MyDrive/STP598 Project dataset/archive/sign_mnist_test.csv')

Train_data.shape

Train_data.head()

Test_data.shape

Test_data.head()

# Separate input features (images) and output labels (sign classes) from training and test datasets

Train_X = np.array(Train_data.drop(columns=['label']))
Test_X = np.array(Test_data.drop(columns=['label']))
Train_Y = np.array(Train_data['label'])
Test_Y = np.array(Test_data['label'])

# Visualize the distribution of sign classes in the training dataset using a bar plot

plt.figure(figsize=(12,6))
sns.countplot(x="label", data=Train_data)

plt.show()

# Reshape input features from 1D arrays to 3D arrays and normalize pixel values between 0 and 1

Train_X = np.array(Train_X).reshape((-1, 28, 28,1)).astype(np.uint8) / 255.0
Test_X = np.array(Test_X).reshape((-1, 28, 28,1)).astype(np.uint8) / 255.0

#Printing data shapes

print("Training data shape - input: ", Train_X.shape)
print("Training data shape - output: ", Train_Y.shape)
print("Testing data shape - input: ", Test_X.shape)
print("Testing data shape - output: ", Test_Y.shape)

#Defining the CNN model

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Conv2D(64, (3, 3), input_shape=(28, 28, 1), activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))

model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))

model.add(tf.keras.layers.Flatten())

model.add(tf.keras.layers.Dense(50, activation='relu'))
model.add(tf.keras.layers.Dense(26, activation='softmax'))


#Compiling the model

model.compile(optimizer='adam',
              loss = 'sparse_categorical_crossentropy',
              metrics=['accuracy'])


model.summary()

history= model.fit(Train_X, Train_Y, epochs=5, validation_data=(Test_X, Test_Y))

#Extracting training history

history_dict = history.history
accuracy = history_dict['accuracy']
validation_accuracy = history_dict['val_accuracy']
loss = history_dict['loss']
validation_loss = history_dict['val_loss']

# Create a range of epochs for plotting
epochs = range(1, len(accuracy) + 1)

# Plot the training and validation accuracy
plt.plot(epochs, accuracy, 'r', label='Training accuracy')
plt.plot(epochs, validation_accuracy, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()
plt.show()

# Plot the training and validation loss
plt.plot(epochs, loss, 'r', label='Training Loss')
plt.plot(epochs, validation_loss, 'b', label='Validation Loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

save_path = "/content/drive/MyDrive/STP598 Project dataset/archive/model.h5"
model.save(save_path)

"""**Confusion Matrix**"""

plt.figure(figsize=(10,10))

# Get the confusion matrix
predict_x = model.predict(Test_X)
y_pred = np.argmax(predict_x, axis = 1)
cm = confusion_matrix(Test_Y, y_pred)

fig, ax = plt.subplots(figsize=(16,16))
# Plot the confusion matrix using matplotlib
import matplotlib.pyplot as plt
import seaborn as sns

sns.heatmap(cm, annot=True, cmap='Blues',fmt='.2f')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""**Testing On Data**"""

fig, axs = plt.subplots(1, 6, figsize=(20, 4))

for i in range(6):
    testImage = Test_X[i]
    prediction = model.predict(testImage.reshape(-1,28,28,1))
    
    axs[i].imshow(testImage.reshape(28, 28))
    axs[i].set_xlabel(f"Prediction: {np.argmax(prediction)}, Actual Value: {Test_Y[i]}")
plt.show()

import keras

# Define optimizer names and learning rates
optimizers = [("Adam Optimizer", tf.keras.optimizers.Adam(learning_rate=0.001)),    
              ("SGD Optimizer", keras.optimizers.SGD(learning_rate=0.001)),    
              ("RMSprop Optimizer", keras.optimizers.RMSprop(learning_rate=0.001))]

# Train the model for each optimizer
history_all_optimizer = []
for optimizer_name, optimizer in optimizers:
    print(f"\n\n The CNN model is initialized with {optimizer_name}")

    # Define the model architecture
    model = keras.Sequential([
        keras.layers.Conv2D(64, (3, 3), input_shape=(28, 28, 1), activation='relu'),
        keras.layers.MaxPooling2D(pool_size=(2, 2)),
        keras.layers.Conv2D(64, (3, 3), activation='relu'),
        keras.layers.MaxPooling2D(pool_size=(2, 2)),
        keras.layers.Flatten(),
        keras.layers.Dense(128, activation='relu'),
        keras.layers.Dense(26, activation='softmax')
    ])

    # Compile the model with the given optimizer
    model.compile(optimizer=optimizer,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    # Train the model
    history = model.fit(Train_X, Train_Y, epochs=5, validation_data=(Test_X, Test_Y))

    history_all_optimizer.append(history)

# model loss for Adam, SGD and RMSprop optimizers
epochs = 5
epoch_range = range(1, epochs+1)
plt.plot(epoch_range, history_all_optimizer[0].history['loss'],  label='Adam')
plt.plot(epoch_range, history_all_optimizer[1].history['loss'],  label='SGD')
plt.plot(epoch_range, history_all_optimizer[2].history['loss'],  label='RMSprop')
plt.title('Model Loss for various Optimizers')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()

# Plot the training and validation accuracy for Adam and RMSprop optimizers

plt.plot(epoch_range, history_all_optimizer[0].history['accuracy'], label='Adam - Training Accuracy')
plt.plot(epoch_range, history_all_optimizer[0].history['val_accuracy'], label='Adam - Validation Accuracy')
plt.plot(epoch_range, history_all_optimizer[2].history['accuracy'], label='RMSprop - Training Accuracy')
plt.plot(epoch_range, history_all_optimizer[2].history['val_accuracy'], label='RMSprop - Validation Accuracy')
plt.title('Model Accuracy for Adam and RMSprop Optimizers')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.show()